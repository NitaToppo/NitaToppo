{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitaToppo/NitaToppo/blob/main/Luhnalgo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eBUbE6uYmjy",
        "outputId": "5d063ac7-e137-4beb-8bb3-e0013fafbd13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from heapq import nlargest\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "zklQnUJ9Zr_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "QB7Z5NLBZivs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate frequency of each word"
      ],
      "metadata": {
        "id": "2-zVsAGfadXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def word_frequencies(text):\n",
        "    word_freq = Counter(text)\n",
        "    max_freq = max(word_freq.values())\n",
        "    for word in word_freq.keys():\n",
        "        word_freq[word] = (word_freq[word]//max_freq)\n",
        "    return word_freq"
      ],
      "metadata": {
        "id": "xsbQDlgqaSz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main function"
      ],
      "metadata": {
        "id": "gtyJuHcNaoNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text, num_sentences):\n",
        "    tokens = preprocess_text(text)\n",
        "    word_freq = word_frequencies(tokens)\n",
        "    sentence_scores = {}\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    for sentence in sentences:\n",
        "        for word in nltk.word_tokenize(sentence.lower()):\n",
        "            if word in word_freq.keys():\n",
        "                if len(sentence.split(' ')) < 30:\n",
        "                    if sentence not in sentence_scores.keys():\n",
        "                        sentence_scores[sentence] = word_freq[word]\n",
        "                    else:\n",
        "                        sentence_scores[sentence] += word_freq[word]\n",
        "    summary_sentences = nlargest(num_sentences, sentence_scores, key=lambda sentence: sentence_scores[sentence])\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "6i2eYldFbmO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human languages. In particular, it focuses on programming computers to process and analyze large amounts of natural language data. NLP has many practical applications, including machine translation, sentiment analysis, and text summarization.\n",
        "Text summarization is the process of reducing a large text document to a shorter version, while retaining the most important information. One of the popular approaches for text summarization is the Luhn algorithm. In this algorithm, the most important sentences are selected based on their frequency of occurrence of important words.\n",
        "Python provides various libraries for natural language processing (NLP) such as NLTK, spaCy, and Gensim. In this article, we will use the NLTK library to implement the Luhn algorithm for text summarization.\n",
        "\"\"\"\n",
        "summary = summarize(text, 2)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgRFgQhRb-VO",
        "outputId": "adc5f987-900a-4255-b086-00bf7a16c771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text summarization is the process of reducing a large text document to a shorter version, while retaining the most important information. NLP has many practical applications, including machine translation, sentiment analysis, and text summarization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"  • Tesla, Inc. is an American multinational automotive and clean energy company.\n",
        "            • Tesla designs and manufactures electric vehicles, stationary battery energy storage devices, solar panels, and solar roof tiles.\n",
        "            • Tesla is one of the world's most valuable companies and led the battery electric vehicle market in 2022 with 18% share.\n",
        "            • Tesla Energy is a major installer of photovoltaic systems in the United States and one of the largest global suppliers of battery energy storage systems.\n",
        "            • Elon Musk became CEO in 2008 and the company's mission is to help expedite the move to sustainable transport and energy through electric vehicles and solar power.\n",
        "            • Tesla has produced several car models, including the Roadster sports car, Model S sedan, Model X SUV, Model 3 sedan, Model Y crossover, and the Tesla Semi truck.\n",
        "            • Tesla's Model 3 is the all\"\"\"\n",
        "\n",
        "summary = summarize(text, 2)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vjM1Km3p9H3",
        "outputId": "02e4fd2c-dc75-436e-8107-1930e8f2fc59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• Tesla has produced several car models, including the Roadster sports car, Model S sedan, Model X SUV, Model 3 sedan, Model Y crossover, and the Tesla Semi truck.   • Tesla, Inc. is an American multinational automotive and clean energy company.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"\"\"without being explicitly programmed or learning without any direct human intervention. This machine learning process starts with feeding them good quality data and then training the machines by building various machine learning models using the data and different algorithms.\n",
        "The choice of algorithms depends on what type of data we have and what kind of task we are trying to automate the machine. As for the formal definition of Machine Learning, we can say that a Machine Learning algorithm learns from experience E with respect to some type of task T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.For example, If a Machine Learning algorithm is used to play chess. Then the experience E is playing many games of chess, the task T is playing chess with many players, and the performance reasure P is the probability that the algorithm will win in the game of chess \"\"\"\n",
        "\n",
        "summary1 = summarize(text1, 2)\n",
        "print(summary1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skc13MKcINIg",
        "outputId": "9dd0ec26-1397-462b-d7ac-294ba1997842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This machine learning process starts with feeding them good quality data and then training the machines by building various machine learning models using the data and different algorithms. without being explicitly programmed or learning without any direct human intervention.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of original text\", len(text1.split(' ')))\n",
        "print(\"length of summary text\", len(summary1.split(' ')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGt7jxAQgb-V",
        "outputId": "a270cc77-f45e-46e9-8af6-98d910e1661c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of original text 155\n",
            "length of summary text 39\n"
          ]
        }
      ]
    }
  ]
}